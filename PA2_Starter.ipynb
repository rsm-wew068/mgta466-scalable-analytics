{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell for markdown formatting assistance. This is not a part of the assignment.\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block} '\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGTA 466: Analytics Assignment 2 - Word Count\n",
    "\n",
    "#### Submission on Gradescope:\n",
    "  * You need to submit the following three files under \"PA2\". Instructions to generate the csv files are given in their respective sections\n",
    "      * The current notebook - **PA2_Starter.ipynb** - with all cells executed\n",
    "      * csv file containing the 100 most frequently occurring words between 5 and 10 characters long (inclusive) and their counts, with columns named `word` and `count` - **100_words.csv**\n",
    "      * csv file containing execution times for 1,2 and 4 cores across three trial, the average and the standard deviation - **exec_time.csv**\n",
    "      \n",
    "#### IMPORTANT submission guidelines enforced by autograder. Please read carefully:\n",
    "  * Make sure that all the cells in this notebook are executed and that the outputs are present in the expected cells before submission\n",
    "  * Some cells are marked **DO NOT DELETE**. These cells cannot be deleted and the output of these cells will be used for autograding\n",
    "  * You can add additional cells, but the **Expected Output** for each of the tasks MUST be the output of the cells marked as such\n",
    "  * DO NOT print anything other than the *exact* expected output. **Do not include any sentences/words describing the output**. This is strictly enforced by the autograder which checks for an *exact* match of the expected output. For example, if you are expected to print the PySpark version:\n",
    "      * '10.9.8' - <span style=\"color:#093\">CORRECT</span>\n",
    "      * 'The PySpark version is 10.9.8' - <span style=\"color:#FF0000\">INCORRECT</span>\n",
    "  * You can add cells for printing debugging information anywhere, but do not print anything else in **Expected Output** cells other than the expected output for the task\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame. \n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "Spark DataFrame Guide:  https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress native-hadoop warning\n",
    "!sed -i '$a\\# Add the line for suppressing the NativeCodeLoader warning \\nlog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR,console' /$HADOOP_HOME/etc/hadoop/log4j.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Copy data file `BookReviews_1M.txt` to the root of HDFS\n",
    "\n",
    "This step is similar to Programming Assignment 1\n",
    "\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start Spark Session\n",
    "\n",
    "##### Change the number of cores in for your program by setting `spark.master` to `local[n]` in this code block where n take values 1,2 and 4.\n",
    "\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the number of cores in this code block\n",
    "# by setting `spark.master` to `local[n]` where\n",
    "# n is the number of cores\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[4]'),\n",
    "                                   ('spark.app.name', 'Basic Setup')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the starting time of execution for timing this notebook\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data\n",
    "\n",
    "Read data from the `BookReviews_1M.txt` file on HDFS\n",
    "\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value=\"This was the first time I read Garcia-Aguilera.  I came upon the name of this book on Live with Regis and Kelly. This book was exactly what I was looking for ... it hit the spot.  I really enjoyed this book because it was well written. Once I started this book it kept me coming back for more. It had culture, family, friendship and romance. I was looking for a little more romance when I picked this book but in the end it turned out to be just right.  I love the main chartachter Margarita (aka Daisy). I've never been to Miami but the way Daisy told the story I certainly felt I'd been there.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textDF = spark.read.text(\"file:///home/jovyan/MGTA466:Session1/Demo1-Local/BookReviews_1M.txt\").cache()\n",
    "textDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean the data - 2 points\n",
    "\n",
    "Task: Remove all punctuations and convert all characters to lower case.\n",
    "\n",
    "Expected output: The first 25 rows of a dataframe, with a column containing the **entire** cleaned sentences. Pass `truncate=False` as argument to `DataFrame.show` to see the entire sentence.\n",
    "\n",
    "#### Your output would look like this, but the entire sentence:\n",
    "\n",
    "|            sentence|\n",
    "----------------------\n",
    "|this was the firs...|\n",
    "|also after going ...|\n",
    "|as with all of ms...|\n",
    "|ive not read any ...|\n",
    "|this romance nove...|\n",
    "|carolina garcia a...|\n",
    "|not only can she ...|\n",
    "|once again garcia...|\n",
    "|the timing is jus...|\n",
    "|engaging dark rea...|\n",
    "|set amid the back...|\n",
    "|this novel is a d...|\n",
    "|if readers are ad...|\n",
    "| reviewed by phyllis|\n",
    "|      apooo bookclub|\n",
    "|a guilty pleasure...|\n",
    "|in the tradition ...|\n",
    "|beryl unger top e...|\n",
    "|what follows is a...|\n",
    "|the book flap say...|\n",
    "|id never before r...|\n",
    "|the novels narrat...|\n",
    "|it is centered on...|\n",
    "|if you like moder...|\n",
    "|beryl unger is a ...|\n",
    "\n",
    "only showing top 25 rows\n",
    "\n",
    "**NOTE** - The above table with cleaned sentences is for illustration only. Your output may differ slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the following function for building a column expression for Task 1. \n",
    "# Do not change this cell. \n",
    "\n",
    "# NOTE: Counterintuitively, column objects do NOT store any data; instead they store column expressions (transformations). \n",
    "#       The below function takes in a column object, and adds more expressions to it to make a more complex transformation. \n",
    "#       Once we have a column object representing the expressions we want, use DataFrame.select(column) to apply the expressions\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\"\"\"\n",
    "    return trim(lower(regexp_replace(column, \"[^A-Za-z0-9 ]\", \"\"))).alias(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'trim(lower(regexp_replace(value, [^A-Za-z0-9 ], , 1))) AS sentence'>\n"
     ]
    }
   ],
   "source": [
    "# Recommended: take a look at the contents of a column object returned from removePunctuations. What's in there? \n",
    "print(removePunctuation(textDF.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF = textDF.select(removePunctuation(col(\"value\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: The first 25 rows of the cleaned dataframe, with a column containing the **entire** cleaned sentences, under a column named `sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "9b98e1e5-2f82-4f63-a9c6-ac5e9a40fec2",
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|this was the first time i read garciaaguilera  i came upon the name of this book on live with regis and kelly this book was exactly what i was looking for  it hit the spot  i really enjoyed this book because it was well written once i started this book it kept me coming back for more it had culture family friendship and romance i was looking for a little more romance when i picked this book but in the end it turned out to be just right  i love the main chartachter margarita aka daisy ive never been to miami but the way daisy told the story i certainly felt id been there                                                                                                                                                                                                                                                                                                                   |\n",
      "|also after going through all of daisys perils  i closed the book with a feeling i had grown emotionally as well                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|as with all of ms garciaaguileras books i think this is a must read impossible to put down successful deviation from past lupe solano seriescaptures the very essence of the excitement local color and diverse fabric of miami sensual and culturally enlightened                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|ive not read any of ms aguileras works before but after having just finished one hot summer im going to check out the lupe solano series ive heard so much about  one hot summer is sooo steamy made me want to move to miami  couldnt put the book down                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|this romance novel is right up there with the rest of her amazing mystery novels  being a guy i was a little hesitant about reading a romance novel but i just had to give this book a shot because i have been such a huge fan of garciaaguileras books  and to be honest i absolutely loved this book  i love the way she presents funky miami and its crazy cubans in not just this book but all her books  garciaaguilera did a superb job with this book and i cant wait till her next book  you gotta read this book                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|carolina garcia aguilera has done it again  shes written another highly enjoyable book and infused it with the right amount of cubanamerican tidbits  my family and i cannot put her books down once we start and this one was not a let down                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|not only can she write mysteriesbut she sure can write a love story this was one hot summer read that i couldnt put down                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|once again garciaaguilera has written a book that i just cant put down i have read and love all her mysteries so i picked up this romance with eager anticipation i was not disappointed the main character margarita is easily likeable which makes the trials she faces that much more intriguing sometimes you want to yell at the book and tell margarita what she should and shouldnt do but the author is measured in the development of the plot and keeps you turning the pages all the way to the end mix that in with an interesting take on cuban exile politics and humorous insights on the world around margarita and the picture of wild steamy miami is complete another winner from garciaaguilera                                                                                                                                                                                                |\n",
      "|the timing is just right for a good book actually its long overdue therefore i highly recommend you read this book i promise you wont be disappointed if this doesnt make the bestsellers list something is definitely wrong here this well written story is so engaging funny almost true etc the character beryl was funny and sad unfortunately there are women just like her and people take advantage of them what we wont do for love penn said women can be so stupid he told her he love her on their first night together and she belived him that was her demise right there                                                                                                                                                                                                                                                                                                                             |\n",
      "|engaging dark reading the book you could almost feel the train wreck about to happen  the collision of characters is painful yet stimulating  files show the depth of her writing in this book especially by showcasing her ethnically diverse cast  it wasnt what i was expecting but if you can get past the initial character introductions youll be hooked  the sex and graphic violence can be overwhelming and hard to stomach but its what makes this bookthe book  if you like tidiness this isnt the book for youits everything but tidy  personally i was slightly disappointed in the final direction of the book but from a literary perspective this is close to a masterpiece  her books just keep getting better and better  this one really pushes the reader mentally and educationally through her use of real literary terms and vocabulary words that may require a quick tour through websters|\n",
      "|set amid the backdrop of new york citys impervious and highly competitive literary world a disinherited struggling writer penn hamilton plots his course to stardom  born with dastardly good looks and a genius iq penn seduces the top editor in the business beryl unger  beryl is a homely narcoleptic obsessivecompulsive workaholic who instantly falls for penns charms and his manuscript  the spindoctors are engaged and penn as much as his novel is an overnight sensation  the money power respect and women ensue  when beryl discovers that penn has another lover and threatens to expose him and ruin his budding career penn ends the charade by committing a heinous crime                                                                                                                                                                                                                      |\n",
      "|this novel is a dark comedy filled with cynicism and wit and a touch of satire surrounding literary figures editors agents etc and concepts wagners gesamtkunstwerk  files offers a credible plot behind the evolution of penns ascent as an artist and embodiment of himself as a brand spanning all genres  music fashion literature cinema even legal scandals  the total package  he becomes an infamous opportunistic conniving heartless protagonist  and yet he is still fairly likeable  files keeps the novel fresh and hip  through penn the reader hangs with the in crowd and meets modern icons in todays entertainment arena                                                                                                                                                                                                                                                                         |\n",
      "|if readers are adventurous enough to veer away from the reesy and misty novels and are patient enough to appreciate the rather lengthy passages dedicated to character development the underlying ripples of black humor and the clever infusion of actual literary works and personalities i think files fans may appreciate this book as an enriched departure from her traditional girlfriend type of novels                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|reviewed by phyllis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|apooo bookclub                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|a guilty pleasure of a book with characters so well formed dialogue so entertaining and a story so well plotted that you want to consume it whole gorge yourself on it and then call up everyone you know and love and recommend it to them  that is exactly my reaction to sex lies murder fame by lolita files                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|in the tradition of kurt andersens fictional dissection of the nineties in turn of the century and tom wolfes mining of the excesses of the eighties in the bonfire of the vanities ms files has written a titillating exploration of fame and branding in the twentyfirst century  sex lies murder fame is a wild adventure through a pop culture world spinning on the axis of the nyc publishing industry and all its media related tentacles                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|beryl unger top editor at kittell press sufferer of narcolepsy and obsessive compulsive disorder is on the hunt for him  the one  the man of her dreams  but reader  i beg you  before the past six years of chick lit that has been thrown at us clouds your judgement   please hold back your preconceived notions about reading a book about a woman looking for love  make no mistake  this is a bona fide novel  with no cutesy lit categories to describe it  when beryl finally meets him he comes in the form of one pennbook hamilton a murderous mentally warped ivy league genius that just so happens to have written a novel                                                                                                                                                                                                                                                                          |\n",
      "|what follows is a tale of love murder deceit fame fortune and cross merchandising  it is a mental feast chock full of real time culture witty dialogue fascinating multidimensional characters and bedroom and bathroom stall scenes that would make judith krantz blush                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|the book flap says kanye west has optioned one of ms files other novels for film development and i sincerely hope that the lucky hollywood executives who have or will stumble onto this story take note of beryls marketing genius and cross promote the hell out of ms lolita files  well done                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|id never before read a lolita files book and was unsure what to expect by the title i expected a dreary story about the sexual reawakening of an unimaginative married woman whose sister is sleeping with her husband in a town where an old friend arrives who likes to video tape frank interviews this story is far from that                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|the novels narrative style is attention grabbing and the story itself has a modern feel to it three lead characters run through most of the story and all jockey for lead position at some point in this comedy of flaws rather than errors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|it is centered on the new york publishing world and because it manages to paint such an unflattering picture of the profession its amazing the story got published at all it probably because it is funny and delightfully cynical                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|if you like modern black humor then this book is for you lets just say that the title of the book is fairly accurate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|beryl unger is a narcoleptic who also has obsessive compulsive disorder and just happens to be an editor to a major publishing house gradually over the span of the story you grow in sympathy towards this abrasive powerhouse as shes her own victim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanedDF.show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Get dataframe containing unique words between 5 and 10 characters long (inclusive) and their counts - 5 points\n",
    "\n",
    "#### 5.1 Create a dataframe of words - 2 points\n",
    "\n",
    "#### Tasks:\n",
    "<ol>\n",
    "    <li type = \"a\"> Split each sentence into words based on the delimiter space (' '). </li>\n",
    "    <li type = \"a\"> Put each word in each sentence row into their own rows. Put your results into a new dataframe with a single column named <code>word</code>.\n",
    "</ol>\n",
    "\n",
    "Useful functions - [pyspark.sql.functions.split](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html), [pyspark.sql.functions.explode](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.explode.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: The first 5 rows of the resulting dataframe, with a single column named `word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "cda66361-f2a3-4978-9af7-95020d0a362d",
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, length\n",
    "wordsDF = cleanedDF.select(explode(split(\"sentence\", \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| word|\n",
      "+-----+\n",
      "| this|\n",
      "|  was|\n",
      "|  the|\n",
      "|first|\n",
      "| time|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Filter words that are between 5 and 10 characters long (inclusive) and count them - 3 points\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "<ol>\n",
    "    <li type = \"a\"> Filter the dataframe to contain only words that are between 5 and 10 characters long (inclusive). </li>\n",
    "    <li type = \"a\"> Group rows in the previous dataframe by unique words, then count the rows in each group.\n",
    "</ol>\n",
    "\n",
    "#### Expected output:\n",
    "\n",
    "<ol>\n",
    "    <li type = \"a\"> First 20 rows of the dataframe, where each row contains only one word that is between 5 and 10 characters long (inclusive). The dataframe must not contain empty rows. </li>\n",
    "    <li type = \"a\"> First 20 rows of the dataframe containing unique words and their counts. </li>\n",
    "</ol>\n",
    "\n",
    "##### The output after filtering words between 5 and 10 characters long (inclusive) would look like this:\n",
    "\n",
    "\n",
    "|          word|\n",
    "----------------\n",
    "|first          |\n",
    "|regis         |\n",
    "|kelly           |\n",
    "\n",
    "... 17 more\n",
    "\n",
    "##### The output after grouping unique words and their counts would look like this:\n",
    "\n",
    "\n",
    "|       word|count|\n",
    "------------|------\n",
    "|still    |343  |\n",
    "|those |1    |\n",
    "|degrade  |2008 |\n",
    "\n",
    "... 17 more\n",
    "\n",
    "\n",
    "**NOTE** - The above table with words and counts is for illustration only. Your output should contain all 20 rows for each of the tasks, and your counts may differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: The first 20 rows of a dataframe, where each row contains only one word between 5 and 10 characters long (inclusive), under a column named `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "ccd31b32-6a56-4955-b31d-71d24310a196",
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|     first|\n",
      "|     regis|\n",
      "|     kelly|\n",
      "|   exactly|\n",
      "|   looking|\n",
      "|    really|\n",
      "|   enjoyed|\n",
      "|   because|\n",
      "|   written|\n",
      "|   started|\n",
      "|    coming|\n",
      "|   culture|\n",
      "|    family|\n",
      "|friendship|\n",
      "|   romance|\n",
      "|   looking|\n",
      "|    little|\n",
      "|   romance|\n",
      "|    picked|\n",
      "|    turned|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uni_5_10 = wordsDF.filter((length(col(\"word\")) >= 5) & (length(col(\"word\")) <= 10))\n",
    "uni_5_10.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: First 20 rows of the dataframe containing unique words between 5 and 10 characters long (inclusive) and their counts, under columns named `word` and `count` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "c0bd5521-2a86-41fd-b7e0-b4e5ff331887",
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     still|52574|\n",
      "|     those|22067|\n",
      "|   degrade|  343|\n",
      "| bookshelf|  900|\n",
      "|amazonings|    1|\n",
      "| recognize| 2008|\n",
      "|     inner|  819|\n",
      "|    harder| 1441|\n",
      "|   lyrical|   14|\n",
      "| viewpoint|   37|\n",
      "|     spoil|   84|\n",
      "|  historys|    2|\n",
      "|  everyday| 2493|\n",
      "| meursault|    1|\n",
      "| involving|  142|\n",
      "| connected| 9172|\n",
      "|    spared|   43|\n",
      "|    doubts|  320|\n",
      "|     1970s|  175|\n",
      "|    brands| 4228|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uni_5_10.groupBy(\"word\").count().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sort the word count dataframe in a **descending** order by count - 2 points\n",
    "\n",
    "**CHECK** - The first row would have the maximum count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: First 20 rows of the word count dataframe sorted in **descending** order by counts, with columns named `word` and `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "ce072285-413f-4b15-8432-34cc32090e94",
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sortedDF = uni_5_10.groupBy(\"word\").count().orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      word| count|\n",
      "+----------+------+\n",
      "|     great|195349|\n",
      "|     these|155186|\n",
      "|     would|118028|\n",
      "|     sound|111156|\n",
      "|   quality|106469|\n",
      "|     works|102626|\n",
      "|    camera|101803|\n",
      "|   product| 96848|\n",
      "|     about| 96704|\n",
      "|     price| 93010|\n",
      "|     other| 83774|\n",
      "|    bought| 79714|\n",
      "|     which| 74529|\n",
      "|     there| 71767|\n",
      "|     after| 70339|\n",
      "|    really| 66968|\n",
      "|   because| 63800|\n",
      "|headphones| 62986|\n",
      "|    better| 62669|\n",
      "|    little| 59847|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedDF.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Record the execution time\n",
    "\n",
    "#### **Expected output**: The execution time. No particular value is expected. This will be needed in section 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.26768589019775\n"
     ]
    }
   ],
   "source": [
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Save the sorted word counts to HDFS as a CSV file - 1 point\n",
    "\n",
    "**NOTE**: Spark uses a distributed memory system, and stores working data in fragments known as \"partitions\". This is advantageous when a Spark cluster spans multiple machines, as each machine will only require part of the working data to do its own job. By default, Spark will save each of these data partitions into a individual file to avoid I/O collisions. We want only one output file, so we'll need to fuse all the data into a single partition first. \n",
    "\n",
    "Your task: \n",
    "1. Coalesce the previous dataframe to one partition using `DataFrame.coalesce(1)`. This returns a 1-partition dataframe. This makes sure that all our results will end up in the same csv file. \n",
    "2. Save the 1-partition dataframe to HDFS using the `DataFrame.write.csv(<path>)` method to the root directory of the HDFS, i.e. `hdfs:///<<your-result-file>>.csv`.\n",
    "\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|word      |count |\n",
      "+----------+------+\n",
      "|great     |195349|\n",
      "|these     |155186|\n",
      "|would     |118028|\n",
      "|sound     |111156|\n",
      "|quality   |106469|\n",
      "|works     |102626|\n",
      "|camera    |101803|\n",
      "|product   |96848 |\n",
      "|about     |96704 |\n",
      "|price     |93010 |\n",
      "|other     |83774 |\n",
      "|bought    |79714 |\n",
      "|which     |74529 |\n",
      "|there     |71767 |\n",
      "|after     |70339 |\n",
      "|really    |66968 |\n",
      "|because   |63800 |\n",
      "|headphones|62986 |\n",
      "|better    |62669 |\n",
      "|little    |59847 |\n",
      "|using     |58806 |\n",
      "|cable     |53488 |\n",
      "|still     |52574 |\n",
      "|first     |49388 |\n",
      "|could     |48790 |\n",
      "|recommend |43101 |\n",
      "|player    |42996 |\n",
      "|problem   |42488 |\n",
      "|computer  |40801 |\n",
      "|years     |40236 |\n",
      "|while     |40054 |\n",
      "|radio     |39952 |\n",
      "|speakers  |39497 |\n",
      "|right     |37779 |\n",
      "|without   |37735 |\n",
      "|small     |37447 |\n",
      "|worked    |37176 |\n",
      "|around    |37074 |\n",
      "|another   |36239 |\n",
      "|thing     |35578 |\n",
      "|their     |34649 |\n",
      "|purchased |34581 |\n",
      "|since     |34526 |\n",
      "|perfect   |34008 |\n",
      "|light     |33786 |\n",
      "|didnt     |33415 |\n",
      "|enough    |33063 |\n",
      "|power     |32755 |\n",
      "|music     |32738 |\n",
      "|never     |32424 |\n",
      "|battery   |32401 |\n",
      "|excellent |32209 |\n",
      "|doesnt    |31998 |\n",
      "|before    |31829 |\n",
      "|looking   |31521 |\n",
      "|again     |30822 |\n",
      "|think     |30766 |\n",
      "|money     |30697 |\n",
      "|needed    |30250 |\n",
      "|pictures  |30215 |\n",
      "|problems  |30000 |\n",
      "|should    |29864 |\n",
      "|however   |29600 |\n",
      "|through   |28882 |\n",
      "|found     |28216 |\n",
      "|amazon    |28128 |\n",
      "|mouse     |27765 |\n",
      "|something |26517 |\n",
      "|though    |26462 |\n",
      "|reviews   |26249 |\n",
      "|pretty    |25957 |\n",
      "|filter    |25883 |\n",
      "|system    |25813 |\n",
      "|laptop    |25672 |\n",
      "|going     |25458 |\n",
      "|canon     |25232 |\n",
      "|digital   |25090 |\n",
      "|working   |24984 |\n",
      "|purchase  |24961 |\n",
      "|cheap     |24696 |\n",
      "|happy     |24572 |\n",
      "|where     |24107 |\n",
      "|tried     |24057 |\n",
      "|every     |23986 |\n",
      "|worth     |23732 |\n",
      "|router    |23712 |\n",
      "|people    |23620 |\n",
      "|software  |23257 |\n",
      "|clear     |22731 |\n",
      "|months    |22548 |\n",
      "|device    |22547 |\n",
      "|batteries |22418 |\n",
      "|being     |22124 |\n",
      "|those     |22067 |\n",
      "|different |22004 |\n",
      "|getting   |21941 |\n",
      "|volume    |21939 |\n",
      "|makes     |21891 |\n",
      "|having    |21796 |\n",
      "|wireless  |21791 |\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = sortedDF.select(\"word\", \"count\").limit(100)\n",
    "\n",
    "df_transformed.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_partition = df_transformed.coalesce(1)\n",
    "\n",
    "df_single_partition.write.mode(\"overwrite\").csv(\"hdfs:///Demo1-HDFS/100_words.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant file saved in the step above is actually a folder, which contains individually saved files from each partition of the saved dataframe. <br> <br>\n",
    "Now, use an HDFS command to show the contents of the resulting folder on HDFS from the last step in the cell below. <br>\n",
    "You will need to include ‘!’ before the HDFS command for Jupyter Notebook to recognize it as an operating system command. For instance '! pwd' displays the path name of your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MGTA466:Session1/Demo1-Local\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expected output**: List of files in the result directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "9badd4ba-63a2-42c4-9dc7-8df9ecaef8f3",
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 jovyan supergroup          0 2025-02-25 12:04 hdfs:///Demo1-HDFS/100_words.csv/_SUCCESS\n",
      "-rw-r--r--   1 jovyan supergroup       1323 2025-02-25 12:04 hdfs:///Demo1-HDFS/100_words.csv/part-00000-400a25f7-a065-4d2a-951a-6485fd3d08ba-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls hdfs:///Demo1-HDFS/100_words.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, stop the spark session in the cell below\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "a6eb6cde-6a4f-4a3e-97aa-bc8c0aff6411",
    "deletable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2025-02-25 12:04 /Demo1-HDFS\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -get hdfs:///Demo1-HDFS/100_words.csv/part-00000-400a25f7-a065-4d2a-951a-6485fd3d08ba-c000.csv 100_words.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Copy the results from HDFS to the local file system - 1 point\n",
    "\n",
    "Now that we have our results stored in HDFS, we need to copy it back to the local file system to access it. This process may sound cumbersome, but it is a necessary result of Spark and Hadoop's distributed architecture, and their ability to scale up to arbitrarily large datasets and computing operations. \n",
    "\n",
    "Copying the results from HDFS to the local file system:\n",
    "1. Run an hdfs command in the terminal to list the root directory of the HDFS. You should see the CSV file that you have saved. Counterintuitively, this CSV file is actually a folder, which contains individually saved files from each partition of the saved dataframe (see above for data partitioning).  \n",
    "2. Run another hdfs command to see what's inside the saved folder. Since we made sure to coalesce our dataframe to just one partition, we should expect to find only one saved partition in this folder, saved also as a CSV. Note the name of this file, it should look something like `part-00000-xx.....xx.csv`. \n",
    "3. Now copy the resultant CSV file from HDFS to the current folder on your local file system using an hdfs command in the terminal. You may rename this file to something more interpretable - let's say `results.csv`. \n",
    "4. We want you to submit a CSV containing the first 101 rows of the results file. To do this, use the command `head -n 101 results.csv > 100_words.csv`. You can also do so manually, since CSV files are in plain text. Remember that we want the first 101 lines which would include the header as well - so basically it is header + 100 rows.\n",
    "\n",
    "#### **Expected output**: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Stop HDFS\n",
    "\n",
    "#### **Expected output**: \"Stopping HDFS ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping HDFS ...\n"
     ]
    }
   ],
   "source": [
    "#Stop HDFS\n",
    "!$HADOOP_HOME/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Submission of `exec_times.csv` containing execution times on different number of cores - 2 point\n",
    "\n",
    "#### **Expected output**(in exec_times.csv file) - Execution times on 1,2 and 4 cores, 3 trials for each core count, the mean and standard deviation of execution times for each core count. The submission should follow the exact template shown below\n",
    "\n",
    "**NOTE** - No output is expected in the notebook\n",
    "\n",
    "After writing all of the expected code before this cell, you should set the cofiguration at the beginning of this Notebook in the cell where this code is present:\n",
    "\n",
    "```conf = pyspark.SparkConf().setAll([('spark.master', 'local[1]'), ('spark.app.name', 'Word Count')])```\n",
    "\n",
    "\n",
    "Create a csv file `exec_times.csv` and fill it with the following template:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cores | Runtime_1 | Runtime_2 | Runtime_3 | Mean | Std |\n",
    "--------|-----------|-----------|-----------|------|------\n",
    "|1| ...| ...| ...| ...| ...|... |\n",
    "|2| ...| ...| ...| ...| ...|... |\n",
    "|4| ...| ...| ...| ...| ...|... |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cores",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Runtime_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Runtime_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Runtime_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Std",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c939451e-a963-4b05-b835-334b1258aacb",
       "rows": [
        [
         "0",
         "1",
         "18.97",
         "20.32",
         "21.37",
         "20.22",
         "0.98"
        ],
        [
         "1",
         "2",
         "17.88",
         "18.39",
         "18.43",
         "18.23",
         "0.25"
        ],
        [
         "2",
         "4",
         "19.5",
         "18.92",
         "19.88",
         "19.43",
         "0.39"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cores</th>\n",
       "      <th>Runtime_1</th>\n",
       "      <th>Runtime_2</th>\n",
       "      <th>Runtime_3</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18.97</td>\n",
       "      <td>20.32</td>\n",
       "      <td>21.37</td>\n",
       "      <td>20.22</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17.88</td>\n",
       "      <td>18.39</td>\n",
       "      <td>18.43</td>\n",
       "      <td>18.23</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>19.50</td>\n",
       "      <td>18.92</td>\n",
       "      <td>19.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cores  Runtime_1  Runtime_2  Runtime_3   Mean   Std\n",
       "0      1      18.97      20.32      21.37  20.22  0.98\n",
       "1      2      17.88      18.39      18.43  18.23  0.25\n",
       "2      4      19.50      18.92      19.88  19.43  0.39"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "exec_times = pd.DataFrame({\"Cores\": [1, 2, 4],\n",
    "                          \"Runtime_1\": [18.97, 17.88, 19.50],\n",
    "                          \"Runtime_2\": [20.32, 18.39, 18.92],\n",
    "                          \"Runtime_3\": [21.37, 18.43, 19.88],\n",
    "                          \"Mean\": [20.22, 18.23, 19.43],\n",
    "                          \"Std\": [0.98, 0.25, 0.39]})\n",
    "exec_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Submission of `100_words.csv` - 2 points\n",
    "\n",
    "#### **Expected output**(in the 100_words.csv file) - Top 100 unique words between 5 and 10 characters long (inclusive) and their counts, sorted in descending order.\n",
    "\n",
    "**NOTE** - No output is expected in the notebook\n",
    "\n",
    "The csv file should have two columns, `word` and `count`, in the first line and 100 more lines with the top 100 unique words between 5 and 10 characters long (inclusive) and their counts, sorted in descending order of the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Autograder\n",
    "\n",
    "The autograder will check whether the results that you submit in the `100_words.csv` file matches **exactly** with the expected results or not.\n",
    "\n",
    "The csv file would look something like this:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|       word|count|\n",
    "------------|------\n",
    "|       great| 100000|\n",
    "|       these| 95000|\n",
    "|      would|90000|\n",
    "\n",
    "... 97 more\n",
    "\n",
    "\n",
    "\n",
    "The counts are shown for illustration -- Your counts may differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
